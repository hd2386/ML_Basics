# Maschinelles Lernen Exam Notes

## Task 1: Allgemeines VerstÃ¤ndnis

### 1. Definieren Sie kurz Supervised, Unsupervised und Reinforcement Learning und nennen Sie je ein Beispielverfahren

*ğŸ”´ Supervised Learning:*
- Lernen mit gelabelten Daten: Modell wird auf Paare (input, output) trainiert.
- *Beispiel:* Lineare Regression, Decision Trees, SVM (Support Vector Machine)

*ğŸŸ¡ Unsupervised Learning:*
- Lernen ohne Labels, Muster in Daten entdecken
- *Beispiel:* k-Means Clustering

*ğŸŸ¢ Reinforcement Learning:*
- Lerne durch Interaktion mit einer Umgebung
- Agent lernt durch Belohnung und Bestrafung
- *Beispiel:* Spielstrategie lernen mit Q-Learning

### 2. ErlÃ¤utern Sie den Unterschied zwischen Parametric und Non-Parametric Methoden und geben Sie je ein Beispiel

*âš¡ Parametric:*
- Feste Anzahl von Parametern, unabhÃ¤ngig von DatensatzgrÃ¶ÃŸe
- *Beispiel:* Lineare Regression

*ğŸ“Š Non-Parametric:*
- Anzahl Parameter wÃ¤chst mit DatensatzgrÃ¶ÃŸe
- *Beispiel:* k-Nearest Neighbor (speichert alle Trainingsdaten)


### 3. Beschreiben Sie das Biasâ€“Variance Trade-off und seine Bedeutung fÃ¼r Over-/Underfitting:

![Bias-Variance-Trade-off](https://github.com/user-attachments/assets/92a132a1-9c51-479f-8b48-24937e66b97f)


*ğŸ¯ Bias:* Systematischer Fehler durch zu vereinfachte Modellannahmen

*ğŸ“ˆ Variance:* Empfindlichkeit gegenÃ¼ber Schwankungen in Trainingsdaten

*âš ï¸ Underfitting:* Hoher Bias, niedrige Variance (zu einfaches Modell)

*ğŸ”¥ Overfitting:* Niedriger Bias, hohe Variance (zu komplexes Modell)

### 4. Machine-Learning-Workflow

![machine_learning-workflow](https://github.com/user-attachments/assets/77ae9b80-e9b2-494f-b8a2-fb16eff7c5c1)

### EXTRA: ErlÃ¤utern Sie den Unterschied zwischen Regression und Classification in Supervised Learning und geben Sie je ein Beispiel.

| Aspekt              | Classification                          | Regression                            |
|---------------------|------------------------------------------|----------------------------------------|
| ğŸ¯ Zielvariable     | Kategorisch (z.â€¯B. â€Jaâ€œ / â€Neinâ€œ)        | Numerisch (z.â€¯B. 3.14, 42)             |
| ğŸ§© Ziel             | Daten in Klassen einteilen               | Einen konkreten Zahlenwert vorhersagen |
| ğŸ“ˆ Beispielmodell   | Logistic Regression, Decision Tree       | Lineare Regression, SVR                |
| ğŸ‘¥ Beispiel         | Spam-Erkennung, Bildklassifikation       | Hauspreisvorhersage, Temperatur        |

### EXTRA: ErlÃ¤utern Sie den Unterschied zwischen White Models und Black Box Models
- White Models: Hohe Interpredability (Linear Regression)
- Black Box Models: Hohe Prediction accuracy (Neuronale Netze, GenAI)

---

## Task 2: Classification Metrics


### 1. Gegeben sei die folgende Confusion-Matrix eines binÃ¤ren Klassifikators auf 200 Beispielen:

|              | pred = Pos    | pred = Neg | 
|--------------|---------------|-------------|
| true = Pos   | 50             | 10            
| true = Neg   | 30             | 110           



### Berechnungen:

*a) Accuracy:*

Accuracy = (TP + TN) / (Gesamt)
         = (50 + 110) / 200
         = 160 / 200 = 0.8 = 80%


*b) Precision:*

Precision = TP / (TP + FP)
          = 50 / (50 + 30)
          = 50 / 80 = 0.625 = 62.5%


*c) Recall vs. FPR:*

*ğŸ“Š Recall (TPR):*
- Anteil der korrekt erkannter positiven FÃ¤lle
- TPR = TP / (TP + FN) = 50 / 60 = 83.3%

*âš ï¸ False Positive Rate (FPR):*
- Anteil der falsch erkannter positive unter negativen FÃ¤lle
- FPR = FP / (FP + TN) = 30 / 140 = 21.4%

---

## Task 3: Wahr oder Falsch?

### 1. Random Forest kann kategoriale EingangsgrÃ¶ÃŸen verarbeiten: *âœ… WAHR*
- Splits basieren auf Kategorien, nicht auf numerischen Schwellenwerten

### 2. One-Hot-Codierung bei k-NN fÃ¼hrt zu sinnvollen AbstÃ¤nden: *âŒ FALSCH*
- One-Hot fÃ¼hrt zu konstanten AbstÃ¤nden zwischen verschiedenen Kategorien
- Alle Kategorien haben gleiche euklidische Distanz (âˆš2)

### 3. HÃ¶herer C-Parameter bei SVM erhÃ¶ht Overfitting-Risiko: *âœ… WAHR*
- C kontrolliert Trade-off zwischen Margin-Maximierung und Fehlerminimierung
- HÃ¶heres C â†’ weniger Regularisierung â†’ hÃ¶heres Overfitting-Risiko

### 4. Kernel-Trick benÃ¶tigt explizite Kenntnis der Abbildung: *âŒ FALSCH*
- Kernel-Trick arbeitet nur mit Skalarprodukt im transformierten Raum
- Explizite Transformation ist nicht notwendig

### 5. ROC-AUC = 0.5 bedeutet besser als Raten: *âŒ FALSCH*
- AUC = 0.5 entspricht zufÃ¤lligem Raten
- Bessere Klassifikatoren haben AUC > 0.5

---

## Task 4: Linear Regression

Gegeben ist das Dataset:
| X | Y | 
|---|---|
| 2 | 3 |                 
| 4 | 7 |         
| 6 | 11 |              
| 8 | 15 |   


### a) Formulieren Sie die Strukturgleichung eines linearen Modells (ğ‘“(x) = â‹¯ ):

f(x) = wâ‚€ + wâ‚ Â· x


### b) Mit Startwerten ğ‘¤0 = 0, ğ‘¤1 = 1 fÃ¼hren Sie eine Gradient Descent-Iteration mit Lernrate ğ›¼ = 0.01 durch. Aktualisieren Sie die Gewichte nach: <br>

![Gradient Descent-Iteration](https://github.com/user-attachments/assets/0f4927cb-9715-49fe-9ae6-2ee4eba9481e)


*Startwerte:* wâ‚€ = 0, wâ‚ = 1, Î± = 0.01        

*Schritt 1: Vorhersagen berechnen*

f(2) = 0 + 1Â·2 = 2 <br>
f(4) = 0 + 1Â·4 = 4 <br> 
f(6) = 0 + 1Â·6 = 6 <br>
f(8) = 0 + 1Â·8 = 8 <br>


*Schritt 2: Fehler berechnen*

eâ‚ = f(2) - yâ‚ = 2 - 3 = -1 <br>
eâ‚‚ = f(4) - yâ‚‚ = 4 - 7 = -3 <br>
eâ‚ƒ = f(6) - yâ‚ƒ = 6 - 11 = -5 <br>
eâ‚„ = f(8) - yâ‚„ = 8 - 15 = -7 <br>


*Schritt 3: Gradienten berechnen*

âˆ‚J/âˆ‚wâ‚€ = (1/4) Â· Î£eáµ¢ = (1/4) Â· (-1-3-5-7) = -4 <br>
âˆ‚J/âˆ‚wâ‚ = (1/4) Â· Î£(eáµ¢ Â· xáµ¢) = (1/4) Â· (-1Â·2 + -3Â·4 + -5Â·6 + -7Â·8) = -25


*Schritt 4: Parameter aktualisieren*

wâ‚€ = 0 - 0.01 Â· (-4) = 0.04 <br>
wâ‚ = 1 - 0.01 Â· (-25) = 1.25


---

## Task 5: k-Nearest Neighbor
### 1. Im Onlineshop liegen folgende Daten (Sekunden auf Website ğ‘¡, Score ğ‘ , Label {0,1}) vor:

### Daten:

| Sek. t  | Score s  | Label | 
|---------|--------------|-------------|
| 30      | 0.2          | 0           
| 45      | 0.5          | 1  
| 50      | 0.4          | 1    
| 20      | 0.3          | 0    
| 60      | 0.6          | 1    


*Ein neuer Nutzer kommt mit (ğ‘¡ = 40, ğ‘  = 0.45).*

### a) Berechnen Sie die euklidischen Distanzen:

dâ‚ = âˆš((40-30)Â² + (0.45-0.2)Â²) = âˆš(100 + 0.0625) = 10.003 <br>
dâ‚‚ = âˆš((40-45)Â² + (0.45-0.5)Â²) = âˆš(25 + 0.0025) = 5.001 <br>
dâ‚ƒ = âˆš((40-50)Â² + (0.45-0.4)Â²) = âˆš(100 + 0.0025) = 10.001 <br>
dâ‚„ = âˆš((40-20)Â² + (0.45-0.3)Â²) = âˆš(400 + 0.0225) = 20.001 <br> 
dâ‚… = âˆš((40-60)Â² + (0.45-0.6)Â²) = âˆš(400 + 0.0225) = 20.001 <br>


### b) Klassifizieren Sie fÃ¼r ğ‘˜ = 1,3,5 (Mehrheitsentscheid):

*Sortierte Distanzen:*
1. dâ‚‚ = 5.001 â†’ Label 1
2. dâ‚ƒ = 10.001 â†’ Label 1
3. dâ‚ = 10.003 â†’ Label 0
4. dâ‚„ = 20.001 â†’ Label 0
5. dâ‚… = 20.001 â†’ Label 1

*Ergebnisse:*
- *k=1:* NÃ¤chster Nachbar hat Label 1 â†’ *Klassifikation: 1*
- *k=3:* 3 nÃ¤chste Nachbarn: 1, 1, 0 â†’ Mehrheit: 1 â†’ *Klassifikation: 1*
- *k=5:* Alle Nachbarn: 1, 1, 0, 0, 1 â†’ Mehrheit: 1 â†’ *Klassifikation: 1*

### c) Nachbarschaften-Skizze:

![k-Nearest Neighbor](https://github.com/user-attachments/assets/ff5fe4b4-4efa-4cc9-8ba8-1fc1615b3d8c)


## Task 6: Unsupervised Learning - Clustering

### a) ErlÃ¤utern Sie den k-Means-Algorithmus und seine Konvergenzbedingung:

*Schritte:*
1. *Initialisierung:* k Zentroide zufÃ¤llig platzieren
2. *Assignment:* Jeden Datenpunkt dem nÃ¤chsten Zentroid zuordnen
3. *Update:* Zentroide als Mittelwert der zugeordneten Punkte neu berechnen
4. *Wiederholung:* Schritte 2-3 bis zur Konvergenz

*Konvergenzbedingung:* Zentroide Ã¤ndern sich nicht mehr (oder nur minimal)

### b) Nennen Sie zwei Nachteile von k-Means und schlagen Sie eine Alternative vor:

*ğŸ”´ Nachteile von k-Means:*
- Erfordert Vorgabe von k
- Empfindlich gegenÃ¼ber Initialisierung
- Nur sphÃ¤rische Cluster

*âœ… Alternative: DBSCAN*
- Erkennt Cluster beliebiger Form
- Automatische Bestimmung der Clusteranzahl
- Robust gegenÃ¼ber AusreiÃŸern

### c) Diskutieren Sie, wie die Wahl der Distanzmetrik (z. B. Euclid vs. Manhattan) das Clustering-Ergebnis beeinflusst:

*ğŸ“ Euklidische Distanz:* Bevorzugt sphÃ¤rische Cluster

*ğŸ”„ Manhattan-Distanz:* Weniger empfindlich gegenÃ¼ber AusreiÃŸern, bevorzugt achsenparallele Strukturen

*Wahl abhÃ¤ngig von:* Datencharakteristik und gewÃ¼nschter Clusterform

---

## Task 7: Neuronale Netze

### a) Skizzieren Sie die Architektur eines einfachen CNN fÃ¼r die Bildklassifikation und beschriften Sie die Layer:


Input Image â†’ Conv Layer â†’ Pooling â†’ Conv Layer â†’ Pooling â†’ Flatten â†’ Dense Layer (Softmax)


*Komponenten:*
- *Conv Layer:* Merkmalsextraktion
- *Pooling:* Dimensionsreduktion
- *Dense Layer:* Vollvernetzte Schicht
- *Softmax:* Wahrscheinlichkeitsverteilung

### b) Berechnen Sie die AusgabegrÃ¶ÃŸe eines 5Ã—5-Bilds bei einem 2Ã—3-Filter und Stride 2 (ohne Padding und mit Zero-Padding P=1):

*Input:* 5Ã—5, Filter: 2Ã—3, Stride: 2

*Formel:* Output_size = floor((Input_size - Filter_size + 2Â·Padding) / Stride) + 1

*Ohne Padding (P=0):*

Output_height = floor((5 - 2)/2) + 1 = floor(1.5) + 1 = 2
Output_width = floor((5 - 3)/2) + 1 = floor(1) + 1 = 2
â†’ Ausgabe: 2Ã—2


*Mit Zero-Padding (P=1):*

Output_height = floor((5 + 2Â·1 - 2)/2) + 1 = floor(2.5) + 1 = 3
Output_width = floor((5 + 2Â·1 - 3)/2) + 1 = floor(2) + 1 = 3
â†’ Ausgabe: 3Ã—3


### c) ErklÃ¤ren Sie kurz, warum Parameter Sharing in CNNs wichtig ist:

*ğŸ”„ Gewichtswiederverwendung:* Gleiche Filter werden auf gesamtes Bild angewendet

*âœ… Vorteile:*
- Reduzierte Parameteranzahl
- Translationsinvarianz
- Bessere Generalisierung

*âš¡ Effizienz:* Weniger Parameter zu trainieren, weniger Overfitting-Risiko

---

## Task 8: ML-Engineer-Praxis

*Projekt:* Gesundheitszustand von Pflanzen vorhersagen

### a) Welche Daten (Features, Label) erfassen Sie, und wie verarbeiten Sie sie vor?

*ğŸŒ± Features:*
- *Alter:* numerisch, in Tagen
- *Wasserzufuhr:* numerisch, ml/Tag
- *Blattfarbe:* kategorisch (grÃ¼n, gelb, braun)

*ğŸ¯ Label:* Gesundheitszustand (kategorisch: gesund, krank, kritisch)

*âš™ï¸ Vorverarbeitung:*
- Normalisierung numerischer Features
- One-Hot-Encoding fÃ¼r Blattfarbe
- AusreiÃŸer-Behandlung bei Wasserzufuhr
- Fehlende Werte durch Median ersetzen

### b) Welches Modell setzen Sie ein, und mit welchen Metriken evaluieren Sie es?

*ğŸŒ³ Modell: Random Forest Classifier*
- Kann mit gemischten Datentypen umgehen
- Robust gegenÃ¼ber AusreiÃŸern
- Liefert Feature Importance

*ğŸ“Š Metriken:*
- *Accuracy:* Gesamtperformance
- *Precision/Recall:* fÃ¼r jede Klasse
- *F1-Score:* fÃ¼r unbalancierte Klassen
- *Confusion Matrix:* detaillierte Analyse

### c) Skizzieren Sie den ML-Workflow:

1. Datensammlung
      
2. Explorative Datenanalyse
      
3. Datenvorbereitung
      
4. Train/Validation/Test Split
      
5. Modelltraining
      
6. Hyperparameter-Tuning (Grid Search)
      
7. Modell-Evaluierung
      
8. Deployment
      
9. Monitoring


### d) Diskutieren Sie Quellen mÃ¶glicher Unsicherheiten in den Vorhersagen und geeignete AnsÃ¤tze, um darauf zu reagieren:

*ğŸ”´ Unsicherheitsquellen:*
- Messfehler bei Wasserzufuhr
- Subjektive Blattfarb-Bewertung
- Saisonale Effekte
- UnvollstÃ¤ndige Daten

*âœ… LÃ¶sungsansÃ¤tze:*
- *DatenqualitÃ¤t:* Mehrfachmessungen, standardisierte Protokolle
- *Modell-Ensemble:* Kombination mehrerer Modelle
- *Konfidenzintervalle:* UnsicherheitsschÃ¤tzung der Vorhersagen
- *Kontinuierliches Lernen:* RegelmÃ¤ÃŸiges Nachtraining mit neuen Daten
- *Feature Engineering:* ZusÃ¤tzliche relevante Features sammeln

---

## ğŸ¯ Wichtige Formeln zum Merken:

### Classification Metrics:

- Accuracy = (TP + TN) / (TP + TN + FP + FN)
- Precision = TP / (TP + FP)
- Recall(TPR) = TP / (TP + FN) 
- FPR = FP / (FP + TN)


### CNN Output Size:

- Output_size = floor((Input_size - Filter_size + 2Â·Padding) / Stride) + 1


### Gradient Descent:

- w_new = w_old - Î± Â· âˆ‡J(w)


### k-NN Distance:

- Euclidean: d = âˆš((xâ‚-xâ‚‚)Â² + (yâ‚-yâ‚‚)Â²)
- Manhattan: d = |xâ‚-xâ‚‚| + |yâ‚-yâ‚‚|

